<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>5_pytorch_retinanet.lib.visualize API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>5_pytorch_retinanet.lib.visualize</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import torchvision
import time
import os
import copy
import pdb
import time
import argparse

import sys
import cv2

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, models, transforms

from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \
        UnNormalizer, Normalizer


assert torch.__version__.split(&#39;.&#39;)[0] == &#39;1&#39;

print(&#39;CUDA available: {}&#39;.format(torch.cuda.is_available()))


def main(args=None):
        parser = argparse.ArgumentParser(description=&#39;Simple training script for training a RetinaNet network.&#39;)

        parser.add_argument(&#39;--dataset&#39;, help=&#39;Dataset type, must be one of csv or coco.&#39;)
        parser.add_argument(&#39;--coco_path&#39;, help=&#39;Path to COCO directory&#39;)
        parser.add_argument(&#39;--csv_classes&#39;, help=&#39;Path to file containing class list (see readme)&#39;)
        parser.add_argument(&#39;--csv_val&#39;, help=&#39;Path to file containing validation annotations (optional, see readme)&#39;)

        parser.add_argument(&#39;--model&#39;, help=&#39;Path to model (.pt) file.&#39;)

        parser = parser.parse_args(args)

        if parser.dataset == &#39;coco&#39;:
                dataset_val = CocoDataset(parser.coco_path, set_name=&#39;train2017&#39;, transform=transforms.Compose([Normalizer(), Resizer()]))
        elif parser.dataset == &#39;csv&#39;:
                dataset_val = CSVDataset(train_file=parser.csv_train, class_list=parser.csv_classes, transform=transforms.Compose([Normalizer(), Resizer()]))
        else:
                raise ValueError(&#39;Dataset type not understood (must be csv or coco), exiting.&#39;)

        sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)
        dataloader_val = DataLoader(dataset_val, num_workers=1, collate_fn=collater, batch_sampler=sampler_val)

        retinanet = torch.load(parser.model)

        use_gpu = True

        if use_gpu:
                retinanet = retinanet.cuda()

        retinanet.eval()

        unnormalize = UnNormalizer()

        def draw_caption(image, box, caption):

                b = np.array(box).astype(int)
                cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)
                cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)

        for idx, data in enumerate(dataloader_val):

                with torch.no_grad():
                        st = time.time()
                        scores, classification, transformed_anchors = retinanet(data[&#39;img&#39;].cuda().float())
                        print(&#39;Elapsed time: {}&#39;.format(time.time()-st))
                        idxs = np.where(scores.cpu()&gt;0.5)
                        img = np.array(255 * unnormalize(data[&#39;img&#39;][0, :, :, :])).copy()

                        img[img&lt;0] = 0
                        img[img&gt;255] = 255

                        img = np.transpose(img, (1, 2, 0))

                        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)

                        for j in range(idxs[0].shape[0]):
                                bbox = transformed_anchors[idxs[0][j], :]
                                x1 = int(bbox[0])
                                y1 = int(bbox[1])
                                x2 = int(bbox[2])
                                y2 = int(bbox[3])
                                label_name = dataset_val.labels[int(classification[idxs[0][j]])]
                                draw_caption(img, (x1, y1, x2, y2), label_name)

                                cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)
                                print(label_name)

                        cv2.imshow(&#39;img&#39;, img)
                        cv2.waitKey(0)



if __name__ == &#39;__main__&#39;:
 main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="5_pytorch_retinanet.lib.visualize.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>args=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(args=None):
        parser = argparse.ArgumentParser(description=&#39;Simple training script for training a RetinaNet network.&#39;)

        parser.add_argument(&#39;--dataset&#39;, help=&#39;Dataset type, must be one of csv or coco.&#39;)
        parser.add_argument(&#39;--coco_path&#39;, help=&#39;Path to COCO directory&#39;)
        parser.add_argument(&#39;--csv_classes&#39;, help=&#39;Path to file containing class list (see readme)&#39;)
        parser.add_argument(&#39;--csv_val&#39;, help=&#39;Path to file containing validation annotations (optional, see readme)&#39;)

        parser.add_argument(&#39;--model&#39;, help=&#39;Path to model (.pt) file.&#39;)

        parser = parser.parse_args(args)

        if parser.dataset == &#39;coco&#39;:
                dataset_val = CocoDataset(parser.coco_path, set_name=&#39;train2017&#39;, transform=transforms.Compose([Normalizer(), Resizer()]))
        elif parser.dataset == &#39;csv&#39;:
                dataset_val = CSVDataset(train_file=parser.csv_train, class_list=parser.csv_classes, transform=transforms.Compose([Normalizer(), Resizer()]))
        else:
                raise ValueError(&#39;Dataset type not understood (must be csv or coco), exiting.&#39;)

        sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)
        dataloader_val = DataLoader(dataset_val, num_workers=1, collate_fn=collater, batch_sampler=sampler_val)

        retinanet = torch.load(parser.model)

        use_gpu = True

        if use_gpu:
                retinanet = retinanet.cuda()

        retinanet.eval()

        unnormalize = UnNormalizer()

        def draw_caption(image, box, caption):

                b = np.array(box).astype(int)
                cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)
                cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)

        for idx, data in enumerate(dataloader_val):

                with torch.no_grad():
                        st = time.time()
                        scores, classification, transformed_anchors = retinanet(data[&#39;img&#39;].cuda().float())
                        print(&#39;Elapsed time: {}&#39;.format(time.time()-st))
                        idxs = np.where(scores.cpu()&gt;0.5)
                        img = np.array(255 * unnormalize(data[&#39;img&#39;][0, :, :, :])).copy()

                        img[img&lt;0] = 0
                        img[img&gt;255] = 255

                        img = np.transpose(img, (1, 2, 0))

                        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)

                        for j in range(idxs[0].shape[0]):
                                bbox = transformed_anchors[idxs[0][j], :]
                                x1 = int(bbox[0])
                                y1 = int(bbox[1])
                                x2 = int(bbox[2])
                                y2 = int(bbox[3])
                                label_name = dataset_val.labels[int(classification[idxs[0][j]])]
                                draw_caption(img, (x1, y1, x2, y2), label_name)

                                cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)
                                print(label_name)

                        cv2.imshow(&#39;img&#39;, img)
                        cv2.waitKey(0)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="5_pytorch_retinanet.lib" href="index.html">5_pytorch_retinanet.lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="5_pytorch_retinanet.lib.visualize.main" href="#5_pytorch_retinanet.lib.visualize.main">main</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>