<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>5_pytorch_retinanet.lib.infer_detector API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>5_pytorch_retinanet.lib.infer_detector</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import sys

import collections
import random
import numpy as np

import skimage
import cv2

import torch
import torch.optim as optim
from torchvision import transforms

from retinanet import model
from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \
    Normalizer
from torch.utils.data import DataLoader

from retinanet import coco_eval
from retinanet import csv_eval

class Infer():
    &#39;&#39;&#39;
    Class for main inference

    Args:
        verbose (int): Set verbosity levels
                        0 - Print Nothing
                        1 - Print desired details
    &#39;&#39;&#39;
    def __init__(self, verbose=1):
        self.system_dict = {};
        self.system_dict[&#34;verbose&#34;] = verbose;
        self.system_dict[&#34;local&#34;] = {};
        self.system_dict[&#34;local&#34;][&#34;common_size&#34;] = 512;
        self.system_dict[&#34;local&#34;][&#34;min_side&#34;] = 608;
        self.system_dict[&#34;local&#34;][&#34;max_side&#34;] = 1024;
        self.system_dict[&#34;local&#34;][&#34;mean&#34;] = np.array([[[0.485, 0.456, 0.406]]]);
        self.system_dict[&#34;local&#34;][&#34;std&#34;] = np.array([[[0.229, 0.224, 0.225]]]);
        self.system_dict[&#34;local&#34;][&#34;colors&#34;] = [(39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86), (14, 89, 122),
          (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26), (103, 1, 17),
          (82, 144, 81), (92, 7, 184), (49, 81, 155), (179, 177, 69), (93, 187, 158), (13, 39, 73), (12, 50, 60),
          (16, 179, 33), (112, 69, 165), (15, 139, 63), (33, 191, 159), (182, 173, 32), (34, 113, 133), (90, 135, 34),
          (53, 34, 86), (141, 35, 190), (6, 171, 8), (118, 76, 112), (89, 60, 55), (15, 54, 88), (112, 75, 181),
          (42, 147, 38), (138, 52, 63), (128, 65, 149), (106, 103, 24), (168, 33, 45), (28, 136, 135), (86, 91, 108),
          (52, 11, 76), (142, 6, 189), (57, 81, 168), (55, 19, 148), (182, 101, 89), (44, 65, 179), (1, 33, 26),
          (122, 164, 26), (70, 63, 134), (137, 106, 82), (120, 118, 52), (129, 74, 42), (182, 147, 112), (22, 157, 50),
          (56, 50, 20), (2, 22, 177), (156, 100, 106), (21, 35, 42), (13, 8, 121), (142, 92, 28), (45, 118, 33),
          (105, 118, 30), (7, 185, 124), (46, 34, 146), (105, 184, 169), (22, 18, 5), (147, 71, 73), (181, 64, 91),
          (31, 39, 184), (164, 179, 33), (96, 50, 18), (95, 15, 106), (113, 68, 54), (136, 116, 112), (119, 139, 130),
          (31, 139, 34), (66, 6, 127), (62, 39, 2), (49, 99, 180), (49, 119, 155), (153, 50, 183), (125, 38, 3),
          (129, 87, 143), (49, 87, 40), (128, 62, 120), (73, 85, 148), (28, 144, 118), (29, 9, 24), (175, 45, 108),
          (81, 175, 64), (178, 19, 157), (74, 188, 190), (18, 114, 2), (62, 128, 96), (21, 3, 150), (0, 6, 95),
          (2, 20, 184), (122, 37, 185)]


    def Model(self, model_path=&#34;final_model.pt&#34;):
        &#39;&#39;&#39;
        User function: Selet trained model params

        Args:
            model_path (str): Relative path to the trained model 

        Returns:
            None
        &#39;&#39;&#39;
        self.system_dict[&#34;local&#34;][&#34;model&#34;] = torch.load(model_path)
        if torch.cuda.is_available():
            self.system_dict[&#34;local&#34;][&#34;model&#34;] = self.system_dict[&#34;local&#34;][&#34;model&#34;].cuda();


    def Predict(self, img_path, class_list, vis_threshold = 0.4, output_folder = &#39;Inference&#39;):
        &#39;&#39;&#39;
        User function: Run inference on image and visualize it

        Args:
            img_path (str): Relative path to the image file
            class_list (list): List of classes in the training set
            vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
            output_folder (str): Path to folder where output images will be saved

        Returns:
            tuple: Contaning label IDs, Scores and bounding box locations of predicted objects. 
        &#39;&#39;&#39;
        
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)
        
        image_filename = os.path.basename(img_path)
        img = skimage.io.imread(img_path)
        image = img.astype(np.float32) / 255.;
        image = (image.astype(np.float32) - self.system_dict[&#34;local&#34;][&#34;mean&#34;]) / self.system_dict[&#34;local&#34;][&#34;std&#34;];

        rows, cols, cns = image.shape

        smallest_side = min(rows, cols)

        # rescale the image so the smallest side is min_side
        scale = self.system_dict[&#34;local&#34;][&#34;min_side&#34;] / smallest_side

        # check if the largest side is now greater than max_side, which can happen
        # when images have a large aspect ratio
        largest_side = max(rows, cols)

        if largest_side * scale &gt; self.system_dict[&#34;local&#34;][&#34;max_side&#34;]:
            scale = self.system_dict[&#34;local&#34;][&#34;max_side&#34;]  / largest_side


        # resize the image with the computed scale
        image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))
        rows, cols, cns = image.shape

        pad_w = 32 - rows%32
        pad_h = 32 - cols%32

        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)
        new_image[:rows, :cols, :] = image.astype(np.float32)

        img = torch.from_numpy(new_image)

        with torch.no_grad():
            scores, labels, boxes = self.system_dict[&#34;local&#34;][&#34;model&#34;](img.cuda().permute(2, 0, 1).float().unsqueeze(dim=0));
            boxes /= scale
        
        try:

            if boxes.shape[0] &gt; 0:
                output_image = cv2.imread(img_path)

                for box_id in range(boxes.shape[0]):
                    pred_prob = float(scores[box_id])
                    if pred_prob &lt; vis_threshold:
                        break
                    pred_label = int(labels[box_id])
                    xmin, ymin, xmax, ymax = boxes[box_id, :]
                    color = random.choice(self.system_dict[&#34;local&#34;][&#34;colors&#34;])
                    cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)
                    text_size = cv2.getTextSize(class_list[pred_label] + &#39; : %.2f&#39; % pred_prob, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]

                    cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)
                    cv2.putText(
                        output_image, class_list[pred_label] + &#39; : %.2f&#39; % pred_prob,
                        (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,
                        (255, 255, 255), 1)

            cv2.imwrite(os.path.join(output_folder, image_filename), output_image)
            cv2.imwrite(&#34;output.jpg&#34;, output_image)
            return scores, labels, boxes
        except:
            print(&#39;No Boxes detected&#39;)
            return None
    
    def predict_batch_of_images(self, img_folder, class_list, vis_threshold = 0.4, output_folder=&#39;Inference&#39;):
        &#39;&#39;&#39;
        User function: Run inference on multiple images and visualize them

        Args:
            img_folder (str): Relative path to folder containing all the image files
            class_list (list): List of classes in the training set
            vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
            output_folder (str): Path to folder where output images will be saved

        Returns:
            None 
        &#39;&#39;&#39;
        
        all_filenames = os.listdir(img_folder)
        all_filenames.sort()
        generated_count = 0
        for filename in all_filenames:
            img_path = &#34;{}/{}&#34;.format(img_folder, filename)
            try:
                self.Predict(img_path , class_list, vis_threshold ,output_folder)
                generated_count += 1
            except:
                continue
        print(&#34;Objects detected  for {} images&#34;.format(generated_count))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="5_pytorch_retinanet.lib.infer_detector.Infer"><code class="flex name class">
<span>class <span class="ident">Infer</span></span>
<span>(</span><span>verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for main inference</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code></dt>
<dd>Set verbosity levels
0 - Print Nothing
1 - Print desired details</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Infer():
    &#39;&#39;&#39;
    Class for main inference

    Args:
        verbose (int): Set verbosity levels
                        0 - Print Nothing
                        1 - Print desired details
    &#39;&#39;&#39;
    def __init__(self, verbose=1):
        self.system_dict = {};
        self.system_dict[&#34;verbose&#34;] = verbose;
        self.system_dict[&#34;local&#34;] = {};
        self.system_dict[&#34;local&#34;][&#34;common_size&#34;] = 512;
        self.system_dict[&#34;local&#34;][&#34;min_side&#34;] = 608;
        self.system_dict[&#34;local&#34;][&#34;max_side&#34;] = 1024;
        self.system_dict[&#34;local&#34;][&#34;mean&#34;] = np.array([[[0.485, 0.456, 0.406]]]);
        self.system_dict[&#34;local&#34;][&#34;std&#34;] = np.array([[[0.229, 0.224, 0.225]]]);
        self.system_dict[&#34;local&#34;][&#34;colors&#34;] = [(39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86), (14, 89, 122),
          (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26), (103, 1, 17),
          (82, 144, 81), (92, 7, 184), (49, 81, 155), (179, 177, 69), (93, 187, 158), (13, 39, 73), (12, 50, 60),
          (16, 179, 33), (112, 69, 165), (15, 139, 63), (33, 191, 159), (182, 173, 32), (34, 113, 133), (90, 135, 34),
          (53, 34, 86), (141, 35, 190), (6, 171, 8), (118, 76, 112), (89, 60, 55), (15, 54, 88), (112, 75, 181),
          (42, 147, 38), (138, 52, 63), (128, 65, 149), (106, 103, 24), (168, 33, 45), (28, 136, 135), (86, 91, 108),
          (52, 11, 76), (142, 6, 189), (57, 81, 168), (55, 19, 148), (182, 101, 89), (44, 65, 179), (1, 33, 26),
          (122, 164, 26), (70, 63, 134), (137, 106, 82), (120, 118, 52), (129, 74, 42), (182, 147, 112), (22, 157, 50),
          (56, 50, 20), (2, 22, 177), (156, 100, 106), (21, 35, 42), (13, 8, 121), (142, 92, 28), (45, 118, 33),
          (105, 118, 30), (7, 185, 124), (46, 34, 146), (105, 184, 169), (22, 18, 5), (147, 71, 73), (181, 64, 91),
          (31, 39, 184), (164, 179, 33), (96, 50, 18), (95, 15, 106), (113, 68, 54), (136, 116, 112), (119, 139, 130),
          (31, 139, 34), (66, 6, 127), (62, 39, 2), (49, 99, 180), (49, 119, 155), (153, 50, 183), (125, 38, 3),
          (129, 87, 143), (49, 87, 40), (128, 62, 120), (73, 85, 148), (28, 144, 118), (29, 9, 24), (175, 45, 108),
          (81, 175, 64), (178, 19, 157), (74, 188, 190), (18, 114, 2), (62, 128, 96), (21, 3, 150), (0, 6, 95),
          (2, 20, 184), (122, 37, 185)]


    def Model(self, model_path=&#34;final_model.pt&#34;):
        &#39;&#39;&#39;
        User function: Selet trained model params

        Args:
            model_path (str): Relative path to the trained model 

        Returns:
            None
        &#39;&#39;&#39;
        self.system_dict[&#34;local&#34;][&#34;model&#34;] = torch.load(model_path)
        if torch.cuda.is_available():
            self.system_dict[&#34;local&#34;][&#34;model&#34;] = self.system_dict[&#34;local&#34;][&#34;model&#34;].cuda();


    def Predict(self, img_path, class_list, vis_threshold = 0.4, output_folder = &#39;Inference&#39;):
        &#39;&#39;&#39;
        User function: Run inference on image and visualize it

        Args:
            img_path (str): Relative path to the image file
            class_list (list): List of classes in the training set
            vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
            output_folder (str): Path to folder where output images will be saved

        Returns:
            tuple: Contaning label IDs, Scores and bounding box locations of predicted objects. 
        &#39;&#39;&#39;
        
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)
        
        image_filename = os.path.basename(img_path)
        img = skimage.io.imread(img_path)
        image = img.astype(np.float32) / 255.;
        image = (image.astype(np.float32) - self.system_dict[&#34;local&#34;][&#34;mean&#34;]) / self.system_dict[&#34;local&#34;][&#34;std&#34;];

        rows, cols, cns = image.shape

        smallest_side = min(rows, cols)

        # rescale the image so the smallest side is min_side
        scale = self.system_dict[&#34;local&#34;][&#34;min_side&#34;] / smallest_side

        # check if the largest side is now greater than max_side, which can happen
        # when images have a large aspect ratio
        largest_side = max(rows, cols)

        if largest_side * scale &gt; self.system_dict[&#34;local&#34;][&#34;max_side&#34;]:
            scale = self.system_dict[&#34;local&#34;][&#34;max_side&#34;]  / largest_side


        # resize the image with the computed scale
        image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))
        rows, cols, cns = image.shape

        pad_w = 32 - rows%32
        pad_h = 32 - cols%32

        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)
        new_image[:rows, :cols, :] = image.astype(np.float32)

        img = torch.from_numpy(new_image)

        with torch.no_grad():
            scores, labels, boxes = self.system_dict[&#34;local&#34;][&#34;model&#34;](img.cuda().permute(2, 0, 1).float().unsqueeze(dim=0));
            boxes /= scale
        
        try:

            if boxes.shape[0] &gt; 0:
                output_image = cv2.imread(img_path)

                for box_id in range(boxes.shape[0]):
                    pred_prob = float(scores[box_id])
                    if pred_prob &lt; vis_threshold:
                        break
                    pred_label = int(labels[box_id])
                    xmin, ymin, xmax, ymax = boxes[box_id, :]
                    color = random.choice(self.system_dict[&#34;local&#34;][&#34;colors&#34;])
                    cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)
                    text_size = cv2.getTextSize(class_list[pred_label] + &#39; : %.2f&#39; % pred_prob, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]

                    cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)
                    cv2.putText(
                        output_image, class_list[pred_label] + &#39; : %.2f&#39; % pred_prob,
                        (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,
                        (255, 255, 255), 1)

            cv2.imwrite(os.path.join(output_folder, image_filename), output_image)
            cv2.imwrite(&#34;output.jpg&#34;, output_image)
            return scores, labels, boxes
        except:
            print(&#39;No Boxes detected&#39;)
            return None
    
    def predict_batch_of_images(self, img_folder, class_list, vis_threshold = 0.4, output_folder=&#39;Inference&#39;):
        &#39;&#39;&#39;
        User function: Run inference on multiple images and visualize them

        Args:
            img_folder (str): Relative path to folder containing all the image files
            class_list (list): List of classes in the training set
            vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
            output_folder (str): Path to folder where output images will be saved

        Returns:
            None 
        &#39;&#39;&#39;
        
        all_filenames = os.listdir(img_folder)
        all_filenames.sort()
        generated_count = 0
        for filename in all_filenames:
            img_path = &#34;{}/{}&#34;.format(img_folder, filename)
            try:
                self.Predict(img_path , class_list, vis_threshold ,output_folder)
                generated_count += 1
            except:
                continue
        print(&#34;Objects detected  for {} images&#34;.format(generated_count))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="5_pytorch_retinanet.lib.infer_detector.Infer.Model"><code class="name flex">
<span>def <span class="ident">Model</span></span>(<span>self, model_path='final_model.pt')</span>
</code></dt>
<dd>
<div class="desc"><p>User function: Selet trained model params</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Relative path to the trained model </dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Model(self, model_path=&#34;final_model.pt&#34;):
    &#39;&#39;&#39;
    User function: Selet trained model params

    Args:
        model_path (str): Relative path to the trained model 

    Returns:
        None
    &#39;&#39;&#39;
    self.system_dict[&#34;local&#34;][&#34;model&#34;] = torch.load(model_path)
    if torch.cuda.is_available():
        self.system_dict[&#34;local&#34;][&#34;model&#34;] = self.system_dict[&#34;local&#34;][&#34;model&#34;].cuda();</code></pre>
</details>
</dd>
<dt id="5_pytorch_retinanet.lib.infer_detector.Infer.Predict"><code class="name flex">
<span>def <span class="ident">Predict</span></span>(<span>self, img_path, class_list, vis_threshold=0.4, output_folder='Inference')</span>
</code></dt>
<dd>
<div class="desc"><p>User function: Run inference on image and visualize it</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Relative path to the image file</dd>
<dt><strong><code>class_list</code></strong> :&ensp;<code>list</code></dt>
<dd>List of classes in the training set</dd>
<dt><strong><code>vis_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold for predicted scores. Scores for objects detected below this score will not be displayed </dd>
<dt><strong><code>output_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to folder where output images will be saved</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Contaning label IDs, Scores and bounding box locations of predicted objects.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Predict(self, img_path, class_list, vis_threshold = 0.4, output_folder = &#39;Inference&#39;):
    &#39;&#39;&#39;
    User function: Run inference on image and visualize it

    Args:
        img_path (str): Relative path to the image file
        class_list (list): List of classes in the training set
        vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
        output_folder (str): Path to folder where output images will be saved

    Returns:
        tuple: Contaning label IDs, Scores and bounding box locations of predicted objects. 
    &#39;&#39;&#39;
    
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    image_filename = os.path.basename(img_path)
    img = skimage.io.imread(img_path)
    image = img.astype(np.float32) / 255.;
    image = (image.astype(np.float32) - self.system_dict[&#34;local&#34;][&#34;mean&#34;]) / self.system_dict[&#34;local&#34;][&#34;std&#34;];

    rows, cols, cns = image.shape

    smallest_side = min(rows, cols)

    # rescale the image so the smallest side is min_side
    scale = self.system_dict[&#34;local&#34;][&#34;min_side&#34;] / smallest_side

    # check if the largest side is now greater than max_side, which can happen
    # when images have a large aspect ratio
    largest_side = max(rows, cols)

    if largest_side * scale &gt; self.system_dict[&#34;local&#34;][&#34;max_side&#34;]:
        scale = self.system_dict[&#34;local&#34;][&#34;max_side&#34;]  / largest_side


    # resize the image with the computed scale
    image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))
    rows, cols, cns = image.shape

    pad_w = 32 - rows%32
    pad_h = 32 - cols%32

    new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)
    new_image[:rows, :cols, :] = image.astype(np.float32)

    img = torch.from_numpy(new_image)

    with torch.no_grad():
        scores, labels, boxes = self.system_dict[&#34;local&#34;][&#34;model&#34;](img.cuda().permute(2, 0, 1).float().unsqueeze(dim=0));
        boxes /= scale
    
    try:

        if boxes.shape[0] &gt; 0:
            output_image = cv2.imread(img_path)

            for box_id in range(boxes.shape[0]):
                pred_prob = float(scores[box_id])
                if pred_prob &lt; vis_threshold:
                    break
                pred_label = int(labels[box_id])
                xmin, ymin, xmax, ymax = boxes[box_id, :]
                color = random.choice(self.system_dict[&#34;local&#34;][&#34;colors&#34;])
                cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)
                text_size = cv2.getTextSize(class_list[pred_label] + &#39; : %.2f&#39; % pred_prob, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]

                cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)
                cv2.putText(
                    output_image, class_list[pred_label] + &#39; : %.2f&#39; % pred_prob,
                    (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,
                    (255, 255, 255), 1)

        cv2.imwrite(os.path.join(output_folder, image_filename), output_image)
        cv2.imwrite(&#34;output.jpg&#34;, output_image)
        return scores, labels, boxes
    except:
        print(&#39;No Boxes detected&#39;)
        return None</code></pre>
</details>
</dd>
<dt id="5_pytorch_retinanet.lib.infer_detector.Infer.predict_batch_of_images"><code class="name flex">
<span>def <span class="ident">predict_batch_of_images</span></span>(<span>self, img_folder, class_list, vis_threshold=0.4, output_folder='Inference')</span>
</code></dt>
<dd>
<div class="desc"><p>User function: Run inference on multiple images and visualize them</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>Relative path to folder containing all the image files</dd>
<dt><strong><code>class_list</code></strong> :&ensp;<code>list</code></dt>
<dd>List of classes in the training set</dd>
<dt><strong><code>vis_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold for predicted scores. Scores for objects detected below this score will not be displayed </dd>
<dt><strong><code>output_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to folder where output images will be saved</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_batch_of_images(self, img_folder, class_list, vis_threshold = 0.4, output_folder=&#39;Inference&#39;):
    &#39;&#39;&#39;
    User function: Run inference on multiple images and visualize them

    Args:
        img_folder (str): Relative path to folder containing all the image files
        class_list (list): List of classes in the training set
        vis_threshold (float): Threshold for predicted scores. Scores for objects detected below this score will not be displayed 
        output_folder (str): Path to folder where output images will be saved

    Returns:
        None 
    &#39;&#39;&#39;
    
    all_filenames = os.listdir(img_folder)
    all_filenames.sort()
    generated_count = 0
    for filename in all_filenames:
        img_path = &#34;{}/{}&#34;.format(img_folder, filename)
        try:
            self.Predict(img_path , class_list, vis_threshold ,output_folder)
            generated_count += 1
        except:
            continue
    print(&#34;Objects detected  for {} images&#34;.format(generated_count))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="5_pytorch_retinanet.lib" href="index.html">5_pytorch_retinanet.lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="5_pytorch_retinanet.lib.infer_detector.Infer" href="#5_pytorch_retinanet.lib.infer_detector.Infer">Infer</a></code></h4>
<ul class="">
<li><code><a title="5_pytorch_retinanet.lib.infer_detector.Infer.Model" href="#5_pytorch_retinanet.lib.infer_detector.Infer.Model">Model</a></code></li>
<li><code><a title="5_pytorch_retinanet.lib.infer_detector.Infer.Predict" href="#5_pytorch_retinanet.lib.infer_detector.Infer.Predict">Predict</a></code></li>
<li><code><a title="5_pytorch_retinanet.lib.infer_detector.Infer.predict_batch_of_images" href="#5_pytorch_retinanet.lib.infer_detector.Infer.predict_batch_of_images">predict_batch_of_images</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>